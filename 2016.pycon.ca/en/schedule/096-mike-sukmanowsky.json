{
  "date": "2016-11-13", 
  "description": "<p>Imagine writing a Python program that could just as easily process a few gigabytes of data locally or hundreds of petabytes in a distributed cluster without changing a single line of code? Too good to be true? It isn't, it's PySpark! In this tutorial we'll learn how to write PySpark that perform basic analysis and fancy machine learning and can run on your computer or thousands of servers.</p>\n<h2>Mike Sukmanowsky Bio</h2>\n<p>Mike is the VP of Product at Parse.ly, a Python-built, analytics company that helps people understand, grow and nurture an audience around digital content.</p>\n<p>Don't let the title fool you, in addition to product management, Mike also hacks. In addition to open source contributions, he has spent the last 4.5 years contributing to Parse.ly's front-end API and dashboard products as well as back-end data collection and processing systems. Prior to Parse.ly, Mike led analytics teams at Canadian media companies. He holds a H.BSc in Computer Science from Wilfred Laurier University.</p>", 
  "end_time": "13:00:00", 
  "github": "msukmanowsky", 
  "github_link": "h", 
  "kind": "talk", 
  "pk": "96", 
  "rooms": "Tutorial", 
  "rowspan": "2", 
  "speakers": "Mike Sukmanowsky", 
  "start_time": "11:50:00", 
  "title": "From gigabytes to petabytes and beyond with PySpark", 
  "twitter": "msukmanowsky"
}
